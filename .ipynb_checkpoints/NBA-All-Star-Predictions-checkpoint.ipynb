{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All-Star</th>\n",
       "      <th>Position</th>\n",
       "      <th>Player</th>\n",
       "      <th>Age</th>\n",
       "      <th>Conference</th>\n",
       "      <th>PPG</th>\n",
       "      <th>RPG</th>\n",
       "      <th>APG</th>\n",
       "      <th>BPG</th>\n",
       "      <th>SPG</th>\n",
       "      <th>GP</th>\n",
       "      <th>W%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Bradley Beal 2018-19</td>\n",
       "      <td>25</td>\n",
       "      <td>East</td>\n",
       "      <td>24.7255</td>\n",
       "      <td>5.0588</td>\n",
       "      <td>5.0980</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>1.3725</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Stephen Curry 2018-19</td>\n",
       "      <td>30</td>\n",
       "      <td>West</td>\n",
       "      <td>29.5500</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>5.4000</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>1.1750</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.7059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>James Harden 2018-19</td>\n",
       "      <td>29</td>\n",
       "      <td>West</td>\n",
       "      <td>36.3404</td>\n",
       "      <td>6.6596</td>\n",
       "      <td>8.1277</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>2.0851</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Kyrie Irving 2018-19</td>\n",
       "      <td>26</td>\n",
       "      <td>East</td>\n",
       "      <td>23.6512</td>\n",
       "      <td>4.7907</td>\n",
       "      <td>6.9302</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>1.7209</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.6275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Damian Lillard 2018-19</td>\n",
       "      <td>28</td>\n",
       "      <td>West</td>\n",
       "      <td>26.4118</td>\n",
       "      <td>4.5686</td>\n",
       "      <td>6.2549</td>\n",
       "      <td>0.4902</td>\n",
       "      <td>1.0784</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.6154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   All-Star Position                  Player  Age Conference      PPG     RPG  \\\n",
       "0      True        G    Bradley Beal 2018-19   25       East  24.7255  5.0588   \n",
       "1      True        G   Stephen Curry 2018-19   30       West  29.5500  5.1000   \n",
       "2      True        G    James Harden 2018-19   29       West  36.3404  6.6596   \n",
       "3      True        G    Kyrie Irving 2018-19   26       East  23.6512  4.7907   \n",
       "4      True        G  Damian Lillard 2018-19   28       West  26.4118  4.5686   \n",
       "\n",
       "      APG     BPG     SPG    GP      W%  \n",
       "0  5.0980  0.7843  1.3725  51.0  0.4314  \n",
       "1  5.4000  0.3500  1.1750  40.0  0.7059  \n",
       "2  8.1277  0.7234  2.0851  47.0  0.5800  \n",
       "3  6.9302  0.4651  1.7209  43.0  0.6275  \n",
       "4  6.2549  0.4902  1.0784  51.0  0.6154  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('nba_player_data_through_jan.csv')\n",
    "dataset.set_index('Player')\n",
    "dataset = dataset.drop(columns = 'Unnamed: 0')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "dataset_guards = dataset[dataset['Position'] == 'G']\n",
    "dataset_bigs = dataset[dataset['Position'] == 'F/C']\n",
    "x = dataset[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y = dataset['All-Star']\n",
    "x_guards = dataset_guards[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_guards = dataset_guards['All-Star']\n",
    "x_bigs = dataset_bigs[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_bigs = dataset_bigs['All-Star']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.8)\n",
    "x_train_std = std_scaler.fit_transform(x_train)\n",
    "x_test_std = std_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm Performance Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classification_algorithms(algos, class_names, x_train, x_test, y_train, y_test): \n",
    "    for classifier in algos: \n",
    "        model = classifier.fit(x_train, y_train)\n",
    "        preds = model.predict(x_test)\n",
    "        print(classifier)\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print(classification_report(y_test, preds, target_names=class_names))\n",
    "def check_classification_k_fold_cross_validation(kfoldtype, algos, class_names, x_data, y_data):\n",
    "    X, Y = np.array(x_data), np.array(y_data)\n",
    "    for classifier in algos:\n",
    "        cv_total_preds = []\n",
    "        cv_total_real = []\n",
    "        std_pipeline = classifier#make_pipeline(StandardScaler(), classifier)\n",
    "        for train_ind, test_ind in kfoldtype.split(X, Y): \n",
    "            x_tr, x_te = X[train_ind], X[test_ind]\n",
    "            y_tr, y_te = Y[train_ind], y[test_ind]\n",
    "            std_pipeline.fit(x_tr, y_tr)\n",
    "            preds = std_pipeline.predict(x_te)\n",
    "            cv_total_real = np.append(cv_total_real,y_te)\n",
    "            cv_total_preds = np.append(cv_total_preds, preds)\n",
    "        print(classifier)\n",
    "        print(confusion_matrix(cv_total_real, cv_total_preds))\n",
    "        print(classification_report(cv_total_real, cv_total_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[6807   26]\n",
      " [ 623  354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      1.00      0.95      6833\n",
      "    All-Star       0.93      0.36      0.52       977\n",
      "\n",
      "    accuracy                           0.92      7810\n",
      "   macro avg       0.92      0.68      0.74      7810\n",
      "weighted avg       0.92      0.92      0.90      7810\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[6803   30]\n",
      " [ 659  318]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.91      1.00      0.95      6833\n",
      "    All-Star       0.91      0.33      0.48       977\n",
      "\n",
      "    accuracy                           0.91      7810\n",
      "   macro avg       0.91      0.66      0.72      7810\n",
      "weighted avg       0.91      0.91      0.89      7810\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[6784   49]\n",
      " [ 625  352]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      0.99      0.95      6833\n",
      "    All-Star       0.88      0.36      0.51       977\n",
      "\n",
      "    accuracy                           0.91      7810\n",
      "   macro avg       0.90      0.68      0.73      7810\n",
      "weighted avg       0.91      0.91      0.90      7810\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[6814   19]\n",
      " [ 622  355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      1.00      0.96      6833\n",
      "    All-Star       0.95      0.36      0.53       977\n",
      "\n",
      "    accuracy                           0.92      7810\n",
      "   macro avg       0.93      0.68      0.74      7810\n",
      "weighted avg       0.92      0.92      0.90      7810\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[6802   31]\n",
      " [ 624  353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      1.00      0.95      6833\n",
      "    All-Star       0.92      0.36      0.52       977\n",
      "\n",
      "    accuracy                           0.92      7810\n",
      "   macro avg       0.92      0.68      0.74      7810\n",
      "weighted avg       0.92      0.92      0.90      7810\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[6808   25]\n",
      " [ 655  322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.91      1.00      0.95      6833\n",
      "    All-Star       0.93      0.33      0.49       977\n",
      "\n",
      "    accuracy                           0.91      7810\n",
      "   macro avg       0.92      0.66      0.72      7810\n",
      "weighted avg       0.91      0.91      0.89      7810\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[6806   27]\n",
      " [ 622  355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      1.00      0.95      6833\n",
      "    All-Star       0.93      0.36      0.52       977\n",
      "\n",
      "    accuracy                           0.92      7810\n",
      "   macro avg       0.92      0.68      0.74      7810\n",
      "weighted avg       0.92      0.92      0.90      7810\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[6611  222]\n",
      " [ 588  389]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      0.97      0.94      6833\n",
      "    All-Star       0.64      0.40      0.49       977\n",
      "\n",
      "    accuracy                           0.90      7810\n",
      "   macro avg       0.78      0.68      0.72      7810\n",
      "weighted avg       0.88      0.90      0.89      7810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[7205   65]\n",
      " [ 551  426]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.93      0.99      0.96      7270\n",
      "    All-Star       0.87      0.44      0.58       977\n",
      "\n",
      "    accuracy                           0.93      8247\n",
      "   macro avg       0.90      0.71      0.77      8247\n",
      "weighted avg       0.92      0.93      0.91      8247\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[7191   79]\n",
      " [ 646  331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      0.99      0.95      7270\n",
      "    All-Star       0.81      0.34      0.48       977\n",
      "\n",
      "    accuracy                           0.91      8247\n",
      "   macro avg       0.86      0.66      0.71      8247\n",
      "weighted avg       0.90      0.91      0.90      8247\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[7086  184]\n",
      " [ 555  422]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.93      0.97      0.95      7270\n",
      "    All-Star       0.70      0.43      0.53       977\n",
      "\n",
      "    accuracy                           0.91      8247\n",
      "   macro avg       0.81      0.70      0.74      8247\n",
      "weighted avg       0.90      0.91      0.90      8247\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[7200   70]\n",
      " [ 562  415]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.93      0.99      0.96      7270\n",
      "    All-Star       0.86      0.42      0.57       977\n",
      "\n",
      "    accuracy                           0.92      8247\n",
      "   macro avg       0.89      0.71      0.76      8247\n",
      "weighted avg       0.92      0.92      0.91      8247\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[7166  104]\n",
      " [ 561  416]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.93      0.99      0.96      7270\n",
      "    All-Star       0.80      0.43      0.56       977\n",
      "\n",
      "    accuracy                           0.92      8247\n",
      "   macro avg       0.86      0.71      0.76      8247\n",
      "weighted avg       0.91      0.92      0.91      8247\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[7208   62]\n",
      " [ 639  338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.92      0.99      0.95      7270\n",
      "    All-Star       0.84      0.35      0.49       977\n",
      "\n",
      "    accuracy                           0.91      8247\n",
      "   macro avg       0.88      0.67      0.72      8247\n",
      "weighted avg       0.91      0.91      0.90      8247\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[7197   73]\n",
      " [ 544  433]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.93      0.99      0.96      7270\n",
      "    All-Star       0.86      0.44      0.58       977\n",
      "\n",
      "    accuracy                           0.93      8247\n",
      "   macro avg       0.89      0.72      0.77      8247\n",
      "weighted avg       0.92      0.93      0.91      8247\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[6714  556]\n",
      " [ 407  570]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      0.92      0.93      7270\n",
      "    All-Star       0.51      0.58      0.54       977\n",
      "\n",
      "    accuracy                           0.88      8247\n",
      "   macro avg       0.72      0.75      0.74      8247\n",
      "weighted avg       0.89      0.88      0.89      8247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "classifiers.append(LogisticRegression())\n",
    "classifiers.append(KNeighborsClassifier(15))\n",
    "classifiers.append(tree.DecisionTreeClassifier())\n",
    "classifiers.append(RandomForestClassifier())\n",
    "classifiers.append(AdaBoostClassifier())\n",
    "classifiers.append(svm.SVC())\n",
    "classifiers.append(xgboost.XGBClassifier())\n",
    "classifiers.append(GaussianNB())\n",
    "kfold = StratifiedKFold(10, True, 42)\n",
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_guards ,y_guards)\n",
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_bigs ,y_bigs)\n",
    "#Random Forest Seems to have the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors w/ NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORS = 15\n",
    "nca = NeighborhoodComponentsAnalysis(random_state = 42)\n",
    "knn = KNeighborsClassifier(NEIGHBORS)\n",
    "nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
    "nca_pipe.fit(x_train_std, y_train)\n",
    "knn_preds = nca_pipe.predict(x_test_std)\n",
    "target_names = ['Non-All-Star', 'All-Star']\n",
    "print(classification_report(y_test, knn_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search CV To Tune Random Forest Classifier\n",
    "Used as reference: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, num = 1)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10, 20]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 37.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_job...\n",
       "                   iid='deprecated', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 10],\n",
       "                                        'min_samples_split': [2, 5, 10, 20],\n",
       "                                        'n_estimators': [10, 231, 452, 673, 894,\n",
       "                                                         1115, 1336, 1557, 1778,\n",
       "                                                         2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_forest = RandomForestClassifier()\n",
    "tuned_r_forest = RandomizedSearchCV(estimator = r_forest, \n",
    "                               param_distributions = random_grid, scoring='f1', n_iter = 100, cv = 10, \n",
    "                               verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 452, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n",
      "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=10,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=452,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[15047    33]\n",
      " [  207   770]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      0.99     15080\n",
      "    All-Star       0.96      0.79      0.87       977\n",
      "\n",
      "    accuracy                           0.99     16057\n",
      "   macro avg       0.97      0.89      0.93     16057\n",
      "weighted avg       0.98      0.99      0.98     16057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tuned_r_forest.best_params_)\n",
    "check_classification_k_fold_cross_validation(kfold, [tuned_r_forest.best_estimator_], ['Non-All-Star', 'All-Star'], x ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Tuned Random Forest to Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=0, random_state=None, strategy='constant')\n",
      "[[15080     0]\n",
      " [  977     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      1.00      0.97     15080\n",
      "    All-Star       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.94     16057\n",
      "   macro avg       0.47      0.50      0.48     16057\n",
      "weighted avg       0.88      0.94      0.91     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='stratified')\n",
      "[[14198   882]\n",
      " [  926    51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      0.94      0.94     15080\n",
      "    All-Star       0.05      0.05      0.05       977\n",
      "\n",
      "    accuracy                           0.89     16057\n",
      "   macro avg       0.50      0.50      0.50     16057\n",
      "weighted avg       0.88      0.89      0.89     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='prior')\n",
      "[[15080     0]\n",
      " [  977     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      1.00      0.97     15080\n",
      "    All-Star       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.94     16057\n",
      "   macro avg       0.47      0.50      0.48     16057\n",
      "weighted avg       0.88      0.94      0.91     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "[[7494 7586]\n",
      " [ 452  525]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      0.50      0.65     15080\n",
      "    All-Star       0.06      0.54      0.12       977\n",
      "\n",
      "    accuracy                           0.50     16057\n",
      "   macro avg       0.50      0.52      0.38     16057\n",
      "weighted avg       0.89      0.50      0.62     16057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rforestcomp = []\n",
    "rforestcomp.append(DummyClassifier(strategy='constant', constant=0))\n",
    "rforestcomp.append(DummyClassifier('stratified'))\n",
    "rforestcomp.append(DummyClassifier('prior'))\n",
    "rforestcomp.append(DummyClassifier('uniform'))                     \n",
    "check_classification_k_fold_cross_validation(kfold, rforestcomp, ['Non-All-Star', 'All-Star'], x ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions Using Tuned Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_current = pd.read_csv('nba_player_data_through_jan_2019_2020.csv')\n",
    "dataset_current.set_index('Player')\n",
    "dataset_current_guards = dataset_current[dataset_current['Position'] == 'G']\n",
    "dataset_current_bigs = dataset_current[dataset_current['Position'] == 'F/C']\n",
    "x_current_guards = dataset_current_guards[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "x_current_bigs = dataset_current_bigs[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "guards_model = copy.copy(tuned_r_forest.best_estimator_)\n",
    "bigs_model = copy.copy(tuned_r_forest.best_estimator_)\n",
    "guards_model.fit(x_guards, y_guards)\n",
    "bigs_model.fit(x_bigs, y_bigs)\n",
    "dataset_current_guards['All-Star'] = guards_model.predict(x_current_guards)\n",
    "dataset_current_guards = dataset_current_guards[dataset_current_guards['All-Star']]\n",
    "dataset_current_guards.to_csv('nba_all_star_predictions_guards_2019_2020.csv')\n",
    "dataset_current_bigs['All-Star'] = bigs_model.predict(x_current_bigs)\n",
    "dataset_current_bigs = dataset_current_bigs[dataset_current_bigs['All-Star']]\n",
    "dataset_current_bigs.to_csv('nba_all_star_predictions_bigs_2019_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-data-env",
   "language": "python",
   "name": "nba-data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
