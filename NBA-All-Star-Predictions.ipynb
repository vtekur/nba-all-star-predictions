{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All-Star</th>\n",
       "      <th>Position</th>\n",
       "      <th>Player</th>\n",
       "      <th>Age</th>\n",
       "      <th>Conference</th>\n",
       "      <th>PPG</th>\n",
       "      <th>RPG</th>\n",
       "      <th>APG</th>\n",
       "      <th>BPG</th>\n",
       "      <th>SPG</th>\n",
       "      <th>GP</th>\n",
       "      <th>W%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Bradley Beal 2018-19</td>\n",
       "      <td>25</td>\n",
       "      <td>East</td>\n",
       "      <td>24.7255</td>\n",
       "      <td>5.0588</td>\n",
       "      <td>5.0980</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>1.3725</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.4314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Stephen Curry 2018-19</td>\n",
       "      <td>30</td>\n",
       "      <td>West</td>\n",
       "      <td>29.5500</td>\n",
       "      <td>5.1000</td>\n",
       "      <td>5.4000</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>1.1750</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.7059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>James Harden 2018-19</td>\n",
       "      <td>29</td>\n",
       "      <td>West</td>\n",
       "      <td>36.3404</td>\n",
       "      <td>6.6596</td>\n",
       "      <td>8.1277</td>\n",
       "      <td>0.7234</td>\n",
       "      <td>2.0851</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Kyrie Irving 2018-19</td>\n",
       "      <td>26</td>\n",
       "      <td>East</td>\n",
       "      <td>23.6512</td>\n",
       "      <td>4.7907</td>\n",
       "      <td>6.9302</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>1.7209</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.6275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>G</td>\n",
       "      <td>Damian Lillard 2018-19</td>\n",
       "      <td>28</td>\n",
       "      <td>West</td>\n",
       "      <td>26.4118</td>\n",
       "      <td>4.5686</td>\n",
       "      <td>6.2549</td>\n",
       "      <td>0.4902</td>\n",
       "      <td>1.0784</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.6154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   All-Star Position                  Player  Age Conference      PPG     RPG  \\\n",
       "0      True        G    Bradley Beal 2018-19   25       East  24.7255  5.0588   \n",
       "1      True        G   Stephen Curry 2018-19   30       West  29.5500  5.1000   \n",
       "2      True        G    James Harden 2018-19   29       West  36.3404  6.6596   \n",
       "3      True        G    Kyrie Irving 2018-19   26       East  23.6512  4.7907   \n",
       "4      True        G  Damian Lillard 2018-19   28       West  26.4118  4.5686   \n",
       "\n",
       "      APG     BPG     SPG    GP      W%  \n",
       "0  5.0980  0.7843  1.3725  51.0  0.4314  \n",
       "1  5.4000  0.3500  1.1750  40.0  0.7059  \n",
       "2  8.1277  0.7234  2.0851  47.0  0.5800  \n",
       "3  6.9302  0.4651  1.7209  43.0  0.6275  \n",
       "4  6.2549  0.4902  1.0784  51.0  0.6154  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('nba_player_data_through_jan.csv')\n",
    "dataset.set_index('Player')\n",
    "dataset = dataset.drop(columns = 'Unnamed: 0')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "dataset_guards = dataset[dataset['Position'] == 'G']\n",
    "dataset_bigs = dataset[dataset['Position'] == 'F/C']\n",
    "x = dataset[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y = dataset['All-Star']\n",
    "x_guards = dataset_guards[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_guards = dataset_guards['All-Star']\n",
    "x_bigs = dataset_bigs[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_bigs = dataset_bigs['All-Star']\n",
    "#Years in which the average league ppg is >= 105\n",
    "year_ranges = [str(year) for year in range(2016,2019)] + [str(year) for year in range(1980,1993)]\n",
    "#year_ranges = [str(year) for year in range(2000,2019)]\n",
    "dataset_guards_restricted = dataset_guards[dataset_guards['Player'].str.split(' ', expand=True)[2].str.split('-',expand=True)[0].isin(year_ranges)]\n",
    "dataset_bigs_restricted = dataset_bigs[dataset_bigs['Player'].str.split(' ', expand=True)[2].str.split('-',expand=True)[0].isin(year_ranges)]\n",
    "x_guards_restricted = dataset_guards_restricted[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_guards_restricted = dataset_guards_restricted['All-Star']\n",
    "x_bigs_restricted = dataset_bigs_restricted[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "y_bigs_restricted = dataset_bigs_restricted['All-Star']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithm Performance Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_classification_algorithms(algos, class_names, x_train, x_test, y_train, y_test): \n",
    "    for classifier in algos: \n",
    "        model = classifier.fit(x_train, y_train)\n",
    "        preds = model.predict(x_test)\n",
    "        print(classifier)\n",
    "        print(confusion_matrix(y_test, preds))\n",
    "        print(classification_report(y_test, preds, target_names=class_names))\n",
    "def check_classification_k_fold_cross_validation(kfoldtype, algos, class_names, x_data, y_data):\n",
    "    X, Y = np.array(x_data), np.array(y_data)\n",
    "    for classifier in algos:\n",
    "        cv_total_preds = []\n",
    "        cv_total_real = []\n",
    "        std_pipeline = make_pipeline(StandardScaler(), classifier)\n",
    "        for train_ind, test_ind in kfoldtype.split(X, Y): \n",
    "            x_tr, x_te = X[train_ind], X[test_ind]\n",
    "            y_tr, y_te = Y[train_ind], Y[test_ind]\n",
    "            std_pipeline.fit(x_tr, y_tr)\n",
    "            preds = std_pipeline.predict(x_te)\n",
    "            cv_total_real = np.append(cv_total_real,y_te)\n",
    "            cv_total_preds = np.append(cv_total_preds, preds)\n",
    "        print(classifier)\n",
    "        print(confusion_matrix(cv_total_real, cv_total_preds))\n",
    "        print(classification_report(cv_total_real, cv_total_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "classifiers.append(LogisticRegression())\n",
    "classifiers.append(KNeighborsClassifier(15))\n",
    "classifiers.append(tree.DecisionTreeClassifier())\n",
    "classifiers.append(RandomForestClassifier())\n",
    "classifiers.append(AdaBoostClassifier())\n",
    "classifiers.append(svm.SVC())\n",
    "classifiers.append(xgboost.XGBClassifier())\n",
    "classifiers.append(GaussianNB())\n",
    "kfold = StratifiedKFold(10, True, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One model for all players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[14944   136]\n",
      " [  283   694]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.99     15080\n",
      "    All-Star       0.84      0.71      0.77       977\n",
      "\n",
      "    accuracy                           0.97     16057\n",
      "   macro avg       0.91      0.85      0.88     16057\n",
      "weighted avg       0.97      0.97      0.97     16057\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[15009    71]\n",
      " [  315   662]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      1.00      0.99     15080\n",
      "    All-Star       0.90      0.68      0.77       977\n",
      "\n",
      "    accuracy                           0.98     16057\n",
      "   macro avg       0.94      0.84      0.88     16057\n",
      "weighted avg       0.97      0.98      0.97     16057\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[15024    56]\n",
      " [  230   747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      1.00      0.99     15080\n",
      "    All-Star       0.93      0.76      0.84       977\n",
      "\n",
      "    accuracy                           0.98     16057\n",
      "   macro avg       0.96      0.88      0.91     16057\n",
      "weighted avg       0.98      0.98      0.98     16057\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[15049    31]\n",
      " [  227   750]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      0.99     15080\n",
      "    All-Star       0.96      0.77      0.85       977\n",
      "\n",
      "    accuracy                           0.98     16057\n",
      "   macro avg       0.97      0.88      0.92     16057\n",
      "weighted avg       0.98      0.98      0.98     16057\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[14943   137]\n",
      " [  268   709]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.99     15080\n",
      "    All-Star       0.84      0.73      0.78       977\n",
      "\n",
      "    accuracy                           0.97     16057\n",
      "   macro avg       0.91      0.86      0.88     16057\n",
      "weighted avg       0.97      0.97      0.97     16057\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[14987    93]\n",
      " [  259   718]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.99     15080\n",
      "    All-Star       0.89      0.73      0.80       977\n",
      "\n",
      "    accuracy                           0.98     16057\n",
      "   macro avg       0.93      0.86      0.90     16057\n",
      "weighted avg       0.98      0.98      0.98     16057\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[14977   103]\n",
      " [  235   742]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.99     15080\n",
      "    All-Star       0.88      0.76      0.81       977\n",
      "\n",
      "    accuracy                           0.98     16057\n",
      "   macro avg       0.93      0.88      0.90     16057\n",
      "weighted avg       0.98      0.98      0.98     16057\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[14109   971]\n",
      " [   95   882]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.94      0.96     15080\n",
      "    All-Star       0.48      0.90      0.62       977\n",
      "\n",
      "    accuracy                           0.93     16057\n",
      "   macro avg       0.73      0.92      0.79     16057\n",
      "weighted avg       0.96      0.93      0.94     16057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x ,y)\n",
    "#Random Forest Seems to have the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Models for Guards and Bigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[7391   26]\n",
      " [  36  357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       1.00      1.00      1.00      7417\n",
      "    All-Star       0.93      0.91      0.92       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.96      0.95      0.96      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[7394   23]\n",
      " [  40  353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      7417\n",
      "    All-Star       0.94      0.90      0.92       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.97      0.95      0.96      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[7366   51]\n",
      " [  44  349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.99      0.99      7417\n",
      "    All-Star       0.87      0.89      0.88       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.93      0.94      0.94      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[7393   24]\n",
      " [  38  355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      7417\n",
      "    All-Star       0.94      0.90      0.92       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.97      0.95      0.96      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[7386   31]\n",
      " [  40  353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      7417\n",
      "    All-Star       0.92      0.90      0.91       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.96      0.95      0.95      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[7393   24]\n",
      " [  28  365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       1.00      1.00      1.00      7417\n",
      "    All-Star       0.94      0.93      0.93       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.97      0.96      0.96      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[7388   29]\n",
      " [  40  353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      7417\n",
      "    All-Star       0.92      0.90      0.91       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.96      0.95      0.95      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[7185  232]\n",
      " [  14  379]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       1.00      0.97      0.98      7417\n",
      "    All-Star       0.62      0.96      0.75       393\n",
      "\n",
      "    accuracy                           0.97      7810\n",
      "   macro avg       0.81      0.97      0.87      7810\n",
      "weighted avg       0.98      0.97      0.97      7810\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[7573   90]\n",
      " [ 172  412]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      7663\n",
      "    All-Star       0.82      0.71      0.76       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.90      0.85      0.87      8247\n",
      "weighted avg       0.97      0.97      0.97      8247\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[7601   62]\n",
      " [ 220  364]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.97      0.99      0.98      7663\n",
      "    All-Star       0.85      0.62      0.72       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.91      0.81      0.85      8247\n",
      "weighted avg       0.96      0.97      0.96      8247\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[7456  207]\n",
      " [ 182  402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.97      0.97      7663\n",
      "    All-Star       0.66      0.69      0.67       584\n",
      "\n",
      "    accuracy                           0.95      8247\n",
      "   macro avg       0.82      0.83      0.82      8247\n",
      "weighted avg       0.95      0.95      0.95      8247\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[7581   82]\n",
      " [ 189  395]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      7663\n",
      "    All-Star       0.83      0.68      0.74       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.90      0.83      0.86      8247\n",
      "weighted avg       0.97      0.97      0.97      8247\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[7543  120]\n",
      " [ 184  400]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.98      0.98      7663\n",
      "    All-Star       0.77      0.68      0.72       584\n",
      "\n",
      "    accuracy                           0.96      8247\n",
      "   macro avg       0.87      0.83      0.85      8247\n",
      "weighted avg       0.96      0.96      0.96      8247\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[7586   77]\n",
      " [ 180  404]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      7663\n",
      "    All-Star       0.84      0.69      0.76       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.91      0.84      0.87      8247\n",
      "weighted avg       0.97      0.97      0.97      8247\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[7574   89]\n",
      " [ 167  417]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      7663\n",
      "    All-Star       0.82      0.71      0.77       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.90      0.85      0.87      8247\n",
      "weighted avg       0.97      0.97      0.97      8247\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[7070  593]\n",
      " [  51  533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.92      0.96      7663\n",
      "    All-Star       0.47      0.91      0.62       584\n",
      "\n",
      "    accuracy                           0.92      8247\n",
      "   macro avg       0.73      0.92      0.79      8247\n",
      "weighted avg       0.96      0.92      0.93      8247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_guards ,y_guards)\n",
    "#SVC seems to be really good for guards \n",
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_bigs ,y_bigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricting Dataset Based on League Scoring - May More Accurately Reflect Modern Scoring Trends\n",
    "Dataset only contains years where average PPG >= 105 in the 3 point era (https://www.basketball-reference.com/leagues/NBA_stats_per_game.html) - similar to 2019-2020 (110.6 PPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[2697   11]\n",
      " [  21  135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      0.99      2708\n",
      "    All-Star       0.92      0.87      0.89       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.93      0.94      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[2697   11]\n",
      " [  19  137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      0.99      2708\n",
      "    All-Star       0.93      0.88      0.90       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.94      0.95      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[2681   27]\n",
      " [  19  137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.99      0.99      2708\n",
      "    All-Star       0.84      0.88      0.86       156\n",
      "\n",
      "    accuracy                           0.98      2864\n",
      "   macro avg       0.91      0.93      0.92      2864\n",
      "weighted avg       0.98      0.98      0.98      2864\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[2698   10]\n",
      " [  21  135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      0.99      2708\n",
      "    All-Star       0.93      0.87      0.90       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.93      0.95      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[2691   17]\n",
      " [  16  140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.99      0.99      2708\n",
      "    All-Star       0.89      0.90      0.89       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.94      0.95      0.94      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[2697   11]\n",
      " [  14  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      2708\n",
      "    All-Star       0.93      0.91      0.92       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.95      0.96      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[2695   13]\n",
      " [  14  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      2708\n",
      "    All-Star       0.92      0.91      0.91       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.95      0.95      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[2637   71]\n",
      " [   6  150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       1.00      0.97      0.99      2708\n",
      "    All-Star       0.68      0.96      0.80       156\n",
      "\n",
      "    accuracy                           0.97      2864\n",
      "   macro avg       0.84      0.97      0.89      2864\n",
      "weighted avg       0.98      0.97      0.98      2864\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "[[2673   35]\n",
      " [  66  174]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      2708\n",
      "    All-Star       0.83      0.72      0.78       240\n",
      "\n",
      "    accuracy                           0.97      2948\n",
      "   macro avg       0.90      0.86      0.88      2948\n",
      "weighted avg       0.96      0.97      0.96      2948\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
      "                     weights='uniform')\n",
      "[[2686   22]\n",
      " [  91  149]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.97      0.99      0.98      2708\n",
      "    All-Star       0.87      0.62      0.73       240\n",
      "\n",
      "    accuracy                           0.96      2948\n",
      "   macro avg       0.92      0.81      0.85      2948\n",
      "weighted avg       0.96      0.96      0.96      2948\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "[[2635   73]\n",
      " [  69  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.97      0.97      0.97      2708\n",
      "    All-Star       0.70      0.71      0.71       240\n",
      "\n",
      "    accuracy                           0.95      2948\n",
      "   macro avg       0.84      0.84      0.84      2948\n",
      "weighted avg       0.95      0.95      0.95      2948\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "[[2676   32]\n",
      " [  79  161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.97      0.99      0.98      2708\n",
      "    All-Star       0.83      0.67      0.74       240\n",
      "\n",
      "    accuracy                           0.96      2948\n",
      "   macro avg       0.90      0.83      0.86      2948\n",
      "weighted avg       0.96      0.96      0.96      2948\n",
      "\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "[[2670   38]\n",
      " [  62  178]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      2708\n",
      "    All-Star       0.82      0.74      0.78       240\n",
      "\n",
      "    accuracy                           0.97      2948\n",
      "   macro avg       0.90      0.86      0.88      2948\n",
      "weighted avg       0.96      0.97      0.97      2948\n",
      "\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[2673   35]\n",
      " [  56  184]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      2708\n",
      "    All-Star       0.84      0.77      0.80       240\n",
      "\n",
      "    accuracy                           0.97      2948\n",
      "   macro avg       0.91      0.88      0.89      2948\n",
      "weighted avg       0.97      0.97      0.97      2948\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[[2673   35]\n",
      " [  67  173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      2708\n",
      "    All-Star       0.83      0.72      0.77       240\n",
      "\n",
      "    accuracy                           0.97      2948\n",
      "   macro avg       0.90      0.85      0.88      2948\n",
      "weighted avg       0.96      0.97      0.96      2948\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "[[2509  199]\n",
      " [  21  219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      0.93      0.96      2708\n",
      "    All-Star       0.52      0.91      0.67       240\n",
      "\n",
      "    accuracy                           0.93      2948\n",
      "   macro avg       0.76      0.92      0.81      2948\n",
      "weighted avg       0.95      0.93      0.93      2948\n",
      "\n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[2697   11]\n",
      " [  14  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.99      1.00      1.00      2708\n",
      "    All-Star       0.93      0.91      0.92       156\n",
      "\n",
      "    accuracy                           0.99      2864\n",
      "   macro avg       0.96      0.95      0.96      2864\n",
      "weighted avg       0.99      0.99      0.99      2864\n",
      "\n",
      "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[2677   31]\n",
      " [  55  185]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      2708\n",
      "    All-Star       0.86      0.77      0.81       240\n",
      "\n",
      "    accuracy                           0.97      2948\n",
      "   macro avg       0.92      0.88      0.90      2948\n",
      "weighted avg       0.97      0.97      0.97      2948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_guards_restricted ,y_guards_restricted)\n",
    "check_classification_k_fold_cross_validation(kfold, classifiers, ['Non-All-Star', 'All-Star'], x_bigs_restricted ,y_bigs_restricted)\n",
    "guards_model = svm.SVC(C=1, gamma='scale', kernel='rbf')\n",
    "bigs_model = svm.SVC(C=100, gamma=0.01, kernel='rbf')\n",
    "check_classification_k_fold_cross_validation(kfold, [guards_model], ['Non-All-Star', 'All-Star'], x_guards_restricted ,y_guards_restricted)\n",
    "check_classification_k_fold_cross_validation(kfold, [bigs_model], ['Non-All-Star', 'All-Star'], x_bigs_restricted ,y_bigs_restricted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search CV To Tune Random Forest Classifier For General Model\n",
    "Used as reference: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 200, num = 1)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10, 20]\n",
    "min_samples_leaf = [1, 2, 4, 10]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r_forest = RandomForestClassifier()\n",
    "tuned_r_forest = RandomizedSearchCV(estimator = r_forest, \n",
    "                               param_distributions = random_grid, scoring='f1', n_iter = 100, cv = 10, \n",
    "                               verbose=2, random_state=42, n_jobs = -1)\n",
    "tuner_r_forest.fit(x,y)\n",
    "print(tuned_r_forest.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV for seperate guard/big models - Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   45.5s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [2, 1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['rbf', 'linear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [2, 1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf', 'linear']} \n",
    "best_svc_guards = GridSearchCV(estimator=svm.SVC(), param_grid=grid, scoring='f1', cv=10, verbose = 3, n_jobs=-1) \n",
    "best_svc_bigs = GridSearchCV(estimator=svm.SVC(), param_grid=grid, scoring='f1', cv=10, verbose = 3, n_jobs=-1) \n",
    "best_svc_guards.fit(x_guards, y_guards)\n",
    "best_svc_bigs.fit(x_bigs, y_bigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=2, kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[7386   31]\n",
      " [  34  359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       1.00      1.00      1.00      7417\n",
      "    All-Star       0.92      0.91      0.92       393\n",
      "\n",
      "    accuracy                           0.99      7810\n",
      "   macro avg       0.96      0.95      0.96      7810\n",
      "weighted avg       0.99      0.99      0.99      7810\n",
      "\n",
      "SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=2, kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "[[7576   87]\n",
      " [ 170  414]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.98      0.99      0.98      7663\n",
      "    All-Star       0.83      0.71      0.76       584\n",
      "\n",
      "    accuracy                           0.97      8247\n",
      "   macro avg       0.90      0.85      0.87      8247\n",
      "weighted avg       0.97      0.97      0.97      8247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(best_svc_guards.best_estimator_)\n",
    "print(best_svc_bigs.best_estimator_)\n",
    "check_classification_k_fold_cross_validation(kfold, [best_svc_guards.best_estimator_], ['Non-All-Star', 'All-Star'], x_guards ,y_guards)\n",
    "check_classification_k_fold_cross_validation(kfold, [best_svc_bigs.best_estimator_], ['Non-All-Star', 'All-Star'], x_bigs ,y_bigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Tuned Random Forest General Model to Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier(constant=0, random_state=None, strategy='constant')\n",
      "[[15080     0]\n",
      " [  977     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      1.00      0.97     15080\n",
      "    All-Star       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.94     16057\n",
      "   macro avg       0.47      0.50      0.48     16057\n",
      "weighted avg       0.88      0.94      0.91     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='stratified')\n",
      "[[14198   882]\n",
      " [  926    51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      0.94      0.94     15080\n",
      "    All-Star       0.05      0.05      0.05       977\n",
      "\n",
      "    accuracy                           0.89     16057\n",
      "   macro avg       0.50      0.50      0.50     16057\n",
      "weighted avg       0.88      0.89      0.89     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='prior')\n",
      "[[15080     0]\n",
      " [  977     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      1.00      0.97     15080\n",
      "    All-Star       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.94     16057\n",
      "   macro avg       0.47      0.50      0.48     16057\n",
      "weighted avg       0.88      0.94      0.91     16057\n",
      "\n",
      "DummyClassifier(constant=None, random_state=None, strategy='uniform')\n",
      "[[7494 7586]\n",
      " [ 452  525]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-All-Star       0.94      0.50      0.65     15080\n",
      "    All-Star       0.06      0.54      0.12       977\n",
      "\n",
      "    accuracy                           0.50     16057\n",
      "   macro avg       0.50      0.52      0.38     16057\n",
      "weighted avg       0.89      0.50      0.62     16057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rforestcomp = []\n",
    "rforestcomp.append(DummyClassifier(strategy='constant', constant=0))\n",
    "rforestcomp.append(DummyClassifier('stratified'))\n",
    "rforestcomp.append(DummyClassifier('prior'))\n",
    "rforestcomp.append(DummyClassifier('uniform'))                     \n",
    "check_classification_k_fold_cross_validation(kfold, rforestcomp, ['Non-All-Star', 'All-Star'], x ,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions - Ultimately Using Support Vector Classifier Fitted on Restricted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/vtek/nba-data-env/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_current = pd.read_csv('nba_player_data_through_jan_2019_2020.csv')\n",
    "dataset_current.set_index('Player')\n",
    "dataset_current_guards = dataset_current[dataset_current['Position'] == 'G']\n",
    "dataset_current_bigs = dataset_current[dataset_current['Position'] == 'F/C']\n",
    "x_current_guards = dataset_current_guards[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "x_current_bigs = dataset_current_bigs[['PPG', 'RPG', 'APG', 'BPG', 'SPG', 'GP', 'W%']]\n",
    "guards_model = svm.SVC(C=1, gamma='scale', kernel='rbf')\n",
    "bigs_model = svm.SVC(C=100, gamma=0.01, kernel='rbf')\n",
    "guards_model.fit(x_guards_restricted, y_guards_restricted)\n",
    "bigs_model.fit(x_bigs_restricted, y_bigs_restricted)\n",
    "dataset_current_guards['All-Star'] = guards_model.predict(x_current_guards)\n",
    "dataset_current_guards = dataset_current_guards[dataset_current_guards['All-Star']]\n",
    "dataset_current_guards.to_csv('nba_all_star_predictions_guards_2019_2020.csv')\n",
    "dataset_current_bigs['All-Star'] = bigs_model.predict(x_current_bigs)\n",
    "dataset_current_bigs = dataset_current_bigs[dataset_current_bigs['All-Star']]\n",
    "dataset_current_bigs.to_csv('nba_all_star_predictions_bigs_2019_2020.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba-data-env",
   "language": "python",
   "name": "nba-data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
